<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title><strong>Christine Kwon</strong></title>

    <meta name="author" content="Christine Kwon">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/myimage.JPG"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 0;" alt="profile photo" src="images/myimage.JPG" class="hoverZoomLink"></a>
              </td>
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Christine Kwon
                </p>
                <p>
		Welcome to my personal website! I am a fourth year PhD student in the <a href="https://hcii.cmu.edu/">Human Computer Interaction Institute (HCII) </a> at Carnegie Mellon University advised by <a[...]
				</p>
				<p>
		My research interests center on <span style="color:#0066CC; font-weight:700">educational technologies (EdTech) </span> and <span style="color:#0066CC; font-weight:700">learning sciences </span>.[...]
		My work has primarily focused on how <span style="color:#0066CC; font-weight:700">low-infrastructure and contextually aligned technologies </span> can support out-of-school learning for margnali[...]
		Through my research, I aim to work with educational technologies globally and collaborate with other educators and researchers on providing <span style="color:#0066CC; font-weight:700">meaningfu[...]
                </p>
				<p>
		I am highly interested in research collaborations and connecting with those who share my passion for EdTech research! Please feel free to reach out to discuss potential collaborative opportuniti[...]
				</p>
                <p style="text-align:center">
                  <a href="mailto:ckwon2@andrew.cmu.edu">üìß Email</a> &nbsp;/&nbsp;
                  <a href="data/Christine_Kwon_UPDATED_Jan_2026.pdf">üíº CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=MVPjZeYAAAAJ&hl=en">üéì Scholar</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr>
  <td style="padding:16px;width:20%;vertical-align:middle">
    <!-- single static image replacing the swap overlay -->
    <img src='images/L@S.png' alt="Radiance Mesh preview" width="100%">
  </td>
  <td style="padding:8px;width:80%;vertical-align:middle">
    <a href="https://dl.acm.org/doi/abs/10.1145/3698205.3729556">
      <span class="papertitle" style="font-weight:700;font-size:20px;display:block;">Validating a New Approach for Measuring Student Engagement in Remote, Low-Infrastructure Learning Environments</span>
    </a>
    <br>
    Michael W. Asher, <strong> Christine Kwon </strong>, John Stamper, Amy Ogan, Paulo F. Carvalho
    <br>
    <em>Learning@Scale</em>, 2025
    <br>
	<a href="https://dl.acm.org/doi/pdf/10.1145/3698205.3729556"
   		class="paperbox"
   		target="_blank"
   		rel="noopener noreferrer">Paper</a>
    <p></p>
    <p>
	In this paper, we introduce a novel solution for studying real-time student engagement with offline learning technology. We present "Prize Codes" as a novel method for measuring student engagement with mobile-learning broadcasts. 
    </p>
  </td>
</tr>

<tr>
  <td style="padding:16px;width:20%;vertical-align:middle">
    <!-- single static image replacing the swap overlay -->
    <img src='images/ECTEL.jpg' alt="Radiance Mesh preview" width="100%">
  </td>
  <td style="padding:8px;width:80%;vertical-align:middle">
    <a href="https://link.springer.com/chapter/10.1007/978-3-032-03870-8_25">
      <span class="papertitle" style="font-weight:700;font-size:20px;display:block;">Integrating Generative AI into Instructional Design Practice: Effects on Graduate Student Learning and Self-Efficacy</span>
    </a>
    <br>
    Steven Moore, Lydia Eckstein, <strong> Christine Kwon </strong>, John Stamper
    <br>
    <em>ECTEL</em>, 2025
    <br>
	<a href="https://stevenjamesmoore.com/assets/papers/ectel25_full_moore.pdf"
   		class="paperbox"
   		target="_blank"
   		rel="noopener noreferrer">Paper</a>
    <p></p>
    <p>
	This study examines genAI's impact on student learning and self-efficacy within a graduate course where students created eight microlessons incorporating distinct learning science principles through an A/B experimental design.
  </td>
</tr>
	
<tr>
  <td style="padding:16px;width:20%;vertical-align:middle">
    <!-- single static image replacing the swap overlay -->
    <img src='images/ICLS.jpg' alt="Radiance Mesh preview" width="100%">
  </td>
  <td style="padding:8px;width:80%;vertical-align:middle">
    <a href="https://repository.isls.org/handle/1/11333">
      <span class="papertitle" style="font-weight:700;font-size:20px;display:block;">Navigating Local versus Colonial Languages of Instruction in Out-of-School Contexts: Insights from a Randomized Controlled Trial in Uganda</span>
    </a>
    <br>
    <strong> Christine Kwon </strong>, Yuchen Yao, Yuhan Che, John Stamper, Amy Ogan
    <br>
    <em>ICLS</em>, 2025
    <br>
	<a href="https://repository.isls.org/bitstream/1/11333/1/ICLS2025_1639-1643.pdf"
   		class="paperbox"
   		target="_blank"
   		rel="noopener noreferrer">Paper</a>
    <p></p>
    <p>
	We studied a randomized controlled trial (RCT) in a rural region within Northern Uganda, comparing a local versus colonial language as a medium of instruction in an engineering course for out-of-school learners.
  </td>
</tr>

<tr>
  <td style="padding:16px;width:20%;vertical-align:middle">
    <!-- single static image replacing the swap overlay -->
    <img src='images/ICLS2.jpg' alt="Radiance Mesh preview" width="100%">
  </td>
  <td style="padding:8px;width:80%;vertical-align:middle">
    <a href="https://repository.isls.org/handle/1/11963">
      <span class="papertitle" style="font-weight:700;font-size:20px;display:block;">Capturing Collaborative Competency with GPT-4o and ENA</span>
    </a>
    <br>
    Yoonjae Lee, <strong>Christine Kwon</strong>, Sarah Seoh, Gahgene Gweon, John Stamper, Carolyn P Ros√©
    <br>
    <em>CSCL</em>, 2025
    <br>
	<a href="https://repository.isls.org/bitstream/1/11963/1/CSCL2025_72-80.pdf"
   		class="paperbox"
   		target="_blank"
   		rel="noopener noreferrer">Paper</a>
    <p></p>
    <p>
	In this study, we develop CoComTag, an LLM-powered approach using GPT-4o that captures students' collaborative competency. 
  </td>
</tr>

<tr>
  <td style="padding:16px;width:20%;vertical-align:middle">
    <!-- single static image replacing the swap overlay -->
    <img src='images/ICLS2.jpg' alt="Radiance Mesh preview" width="100%">
  </td>
  <td style="padding:8px;width:80%;vertical-align:middle">
    <a href="https://dl.acm.org/doi/full/10.1145/3613904.3642221">
      <span class="papertitle" style="font-weight:700;font-size:20px;display:block;">Investigating Demographics and Motivation in Engineering Education Using Radio and Phone-Based Educational Technologies</span>
    </a>
    <br>
    <strong>Christine Kwon</strong>, Darren Butler, Judith Odili Uchidiuno, John Stamper, Amy Ogan
    <br>
    <em>CHI</em>, 2025
    <br>
	<a href="https://dl.acm.org/doi/pdf/10.1145/3613904.3642221"
   		class="paperbox"
   		target="_blank"
   		rel="noopener noreferrer">Paper</a>
    <p></p>
    <p>
	We analyzed log interaction data from an existing offline radio-and-phone based course to examine how participation was associated with changes in learners‚Äô STEM motivations, engineering mindsets, and income mobility. We further investigated how learner outcomes related to initial motivation, demographic characteristics, and access to technology. 
  </td>
</tr>

    <tr onmouseout="r2r_stop()" onmouseover="r2r_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='r2r_image'><video  width=100% muted autoplay loop>
          <source src="images/r2r.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/r2r.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function r2r_start() {
            document.getElementById('r2r_image').style.opacity = "1";
          }

          function r2r_stop() {
            document.getElementById('r2r_image').style.opacity = "0";
          }
          r2r_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://relight-to-reconstruct.github.io/">
          <span class="papertitle">Generative Multiview Relighting for
3D Reconstruction under Extreme Illumination Variation</span>
        </a>
        <br>
        Hadi Alzayer,
        Philipp Henzler,
				<strong>Jonathan T. Barron</strong>, 
        Jia-Bin Huang,
        Pratul P. Srinivasan, 
        Dor Verbin
        <br>
        <em>CVPR</em>, 2025 <strong>(Highlight)</strong>
        <br>
        <a href="https://relight-to-reconstruct.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2412.15211">arXiv</a>
        <p></p>
        <p>
				Images taken under extreme illumination variation can be made consistent with diffusion, and this enables high-quality 3D reconstruction.
        </p>
      </td>
    </tr>

    <tr onmouseout="simvs_stop()" onmouseover="simvs_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='simvs_image'><video  width=100% muted autoplay loop>
          <source src="images/simvs.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/simvs.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function simvs_start() {
            document.getElementById('simvs_image').style.opacity = "1";
          }

          function simvs_stop() {
            document.getElementById('simvs_image').style.opacity = "0";
          }
          simvs_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://alextrevithick.github.io/simvs/">
          <span class="papertitle">SimVS: Simulating World Inconsistencies for Robust View Synthesis</span>
        </a>
        <br>
        Alex Trevithick,
        Roni Paiss,
        Philipp Henzler,
        Dor Verbin,
        Rundi Wu,
        Hadi Alzayer,
        Ruiqi Gao,
        Ben Poole,
				<strong>Jonathan T. Barron</strong>, 
        Aleksander Holynski,
        Ravi Ramamoorthi,
        Pratul P. Srinivasan
        <br>
        <em>CVPR</em>, 2025
        <br>
        <a href="https://alextrevithick.github.io/simvs/">project page</a>
        /
        <a href="https://arxiv.org/abs/2412.07696">arXiv</a>
        <p></p>
        <p>
        Simulating the world with video models lets you make inconsistent captures consistent.
        </p>
      </td>
    </tr>

    <tr onmouseout="power_stop()" onmouseover="power_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='power_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/power.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/power.png' width="160">
        </div>
        <script type="text/javascript">
          function power_start() {
            document.getElementById('power_image').style.opacity = "1";
          }

          function power_stop() {
            document.getElementById('power_image').style.opacity = "0";
          }
          power_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://x.com/jon_barron/status/1891918200931061996">
			<span class="papertitle">A Power Transform
</span>
        </a>
        <br>
				<strong>Jonathan T. Barron</strong>
        <br>
        <em>arXiv</em>, 2025
        <br>
        <a href="https://x.com/jon_barron/status/1891918200931061996">tweet</a>
        /
        <a href="https://arxiv.org/abs/2502.10647">arXiv</a>
        <p></p>
        <p>
				A slight tweak to the Box-Cox power transform generalizes a variety of curves, losses, kernel functions, probability distributions, bump functions, and neural network activation functions.
        </p>
      </td>
    </tr>

    <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cat3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/cat3d.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cat3d.jpg' width="160">
        </div>
        <script type="text/javascript">
          function cat3d_start() {
            document.getElementById('cat3d_image').style.opacity = "1";
          }

          function cat3d_stop() {
            document.getElementById('cat3d_image').style.opacity = "0";
          }
          cat3d_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://cat3d.github.io/">
			<span class="papertitle">CAT3D: Create Anything in 3D with Multi-View Diffusion Models
</span>
        </a>
        <br>
				Ruiqi Gao*,
        Aleksander Holynski*, 
        Philipp Henzler,
        Arthur Brussee, 
				Ricardo Martin Brualla, 
        Pratul P. Srinivasan,
				<strong>Jonathan T. Barron</strong>,
        Ben Poole*
 
        <br>
        <em>NeurIPS</em>, 2024 &nbsp <strong>(Oral Presentation)</strong>
        <br>
        <a href="https://cat3d.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2405.10314">arXiv</a>
        <p></p>
        <p>
				A single model built around diffusion and NeRF that does text-to-3D, image-to-3D, and few-view reconstruction, trains in 1 minute, and renders at 60FPS in a browser.
        </p>
      </td>
    </tr>

    <tr onmouseout="nerfcasting_stop()" onmouseover="nerfcasting_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nerfcasting_image'><video  width=100% muted autoplay loop>
          <source src="images/nerfcasting.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/nerfcasting.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nerfcasting_start() {
            document.getElementById('nerfcasting_image').style.opacity = "1";
          }

          function nerfcasting_stop() {
            document.getElementById('nerfcasting_image').style.opacity = "0";
          }
          nerfcasting_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://nerf-casting.github.io/">
          <span class="papertitle">NeRF-Casting: Improved View-Dependent Appearance with Consistent Reflections</span>
        </a>
        <br>
				
        Dor Verbin,
        Pratul Srinivasan,
				Peter Hedman,
				Benjamin Attal, <br>
				Ben Mildenhall,
				Richard Szeliski,
				<strong>Jonathan T. Barron</strong>
        <br>
        <em>SIGGRAPH Asia</em>, 2024
        <br>
        <a href="https://nerf-casting.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2405.14871">arXiv</a>
        <p></p>
        <p>
        Carefully casting reflection rays lets us synthesize photorealistic specularities in real-world scenes.
        </p>
      </td>
    </tr>

    <tr onmouseout="flash_cache_stop()" onmouseover="flash_cache_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='flash_cache_image'><video  width=100% muted autoplay loop>
          <source src="images/flash_cache_mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/flash_cache.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function flash_cache_start() {
            document.getElementById('flash_cache_image').style.opacity = "1";
          }

          function flash_cache_stop() {
            document.getElementById('flash_cache_image').style.opacity = "0";
          }
          flash_cache_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://benattal.github.io/flash-cache/">
          <span class="papertitle">Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering</span>
        </a>
        <br>
				Benjamin Attal,
        Dor Verbin,
        Ben Mildenhall,
        Peter Hedman, <br>
				<strong>Jonathan T. Barron</strong>,
        Matthew O'Toole,
        Pratul P. Srinivasan
        <br>
        <em>ECCV</em>, 2024 &nbsp <strong>(Oral Presentation)</strong>
        <br>
        <a href="https://benattal.github.io/flash-cache/">project page</a>
        /
        <a href="TODO">arXiv</a>
        <p></p>
        <p>
          A more physically-accurate inverse rendering system based on radiance caching for recovering geometry, materials, and lighting from RGB images of an object or scene.
        </p>
      </td>
    </tr>

    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
          <source src="images/nuvo.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/nuvo.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://pratulsrinivasan.github.io/nuvo/">
          <span class="papertitle">Nuvo: Neural UV Mapping for Unruly 3D Representations</span>
        </a>
        <br>
        Pratul Srinivasan,
        Stephan J. Garbin,
        Dor Verbin,
		<strong>Jonathan T. Barron</strong>,
        Ben Mildenhall
        <br>
        <em>ECCV</em>, 2024
        <br>
        <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        /
        <a href="http://arxiv.org/abs/2312.05283">arXiv</a>
        <p></p>
        <p>
        Neural fields let you recover editable UV mappings for the challenging geometries produced by NeRF-like models.
        </p>
      </td>
    </tr>

    <tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bog_image'><video  width=100% muted autoplay loop>
          <source src="images/bog.jpg" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/bog.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function bog_start() {
            document.getElementById('bog_image').style.opacity = "1";
          }

          function bog_stop() {
            document.getElementById('bog_image').style.opacity = "0";
          }
          bog_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://creiser.github.io/binary_opacity_grid/">
          <span class="papertitle">Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis
</span>
        </a>
        <br>
				Christian Reiser,
				Stephan J. Garbin,
				Pratul Srinivasan,
				Dor Verbin,
				Richard Szeliski,
				Ben Mildenhall,
				<strong>Jonathan T. Barron</strong>,
				Peter Hedman*,
				Andreas Geiger*		
        <br>
        <em>SIGGRAPH</em>, 2024
        <br>
        <a href="https://creiser.github.io/binary_opacity_grid/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=2TPUmGRg8bM">video</a>
        /
        <a href="https://arxiv.org/abs/2402.12377">arXiv</a>
        <p></p>
        <p>
        Applying anti-aliasing to a discrete opacity grid lets you render a hard representation into a soft image, and this enables highly-detailed mesh recovery.
        </p>
      </td>
    </tr>

    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smerf_image'><video  width=100% muted autoplay loop>
          <source src="images/smerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/smerf.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function smerf_start() {
            document.getElementById('smerf_image').style.opacity = "1";
          }

          function smerf_stop() {
            document.getElementById('smerf_image').style.opacity = "0";
          }
          smerf_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://smerf-3d.github.io/">
          <span class="papertitle">SMERF: Streamable Memory Efficient Radiance Fields for Real-Time Large-Scene Exploration</span>
        </a>
        <br>
		Daniel Duckworth*,
		Peter Hedman*,
		Christian Reiser,
		Peter Zhizhin,
		Jean-Fran√ßois Thibert,
        Mario Luƒçiƒá,
        Richard Szeliski,
		<strong>Jonathan T. Barron</strong>
        <br>
        <em>SIGGRAPH</em>, 2024 &nbsp <strong>(Honorable Mention)</strong>
        <br>
        <a href="https://smerf-3d.github.io/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
        <p></p>
        <p>
        Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
        </p>
      </td>
    </tr>
	

  <tr onmouseout="eclipse_stop()" onmouseover="eclipse_start()">
    <td style="padding:16px;width:20%;vertical-align:middle">
      <div class="one">
        <div class="two" id='eclipse_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/eclipse_after.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/eclipse_before.jpg' width="160">
      </div>
      <script type="text/javascript">
        function eclipse_start() {
          document.getElementById('eclipse_image').style.opacity = "1";
        }

        function eclipse_stop() {
          document.getElementById('eclipse_image').style.opacity = "0";
        }
        eclipse_stop()
      </script>
    </td>
    <td style="padding:8px;width:80%;vertical-align:middle">
      <a href="https://dorverbin.github.io/eclipse">
        <span class="papertitle">Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</span>
      </a>
      <br>
      Dor Verbin,
      Ben Mildenhall,
      Peter Hedman, <br>
      <strong>Jonathan T. Barron</strong>,
      Todd Zickler,
      Pratul Srinivasan
      <br>
      <em>CVPR</em>, 2024 &nbsp <strong>(Oral Presentation)</strong>
      <br>
      <a href="https://dorverbin.github.io/eclipse">project page</a>
      /
      <a href="https://www.youtube.com/watch?v=amQLGyza3EU">video</a>
      /
      <a href="https://arxiv.org/abs/2305.16321">arXiv</a>
      <p></p>
      <p>
      Shadows cast by unobserved occluders provide a high-frequency cue for recovering illumination and materials.
      </p>
    </td>
  </tr>

  <tr onmouseout="recon_stop()" onmouseover="recon_start()">
    <td style="padding:16px;width:20%;vertical-align:middle">
      <div class="one">
        <div class="two" id='recon_image'><video  width=100% height=100% muted autoplay loop>
        <source src="images/recon.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/recon.jpg' width="160">
      </div>
      <script type="text/javascript">
        function recon_start() {
          document.getElementById('recon_image').style.opacity = "1";
        }

        function recon_stop() {
          document.getElementById('recon_image').style.opacity = "0";
        }
        recon_stop()
      </script>
    </td>
    <td style="padding:8px;width:80%;vertical-align:middle">
      <a href="https://reconfusion.github.io/">
		<span class="papertitle">ReconFusion: 3D Reconstruction with Diffusion Priors</span>
      </a>
      <br>
      Rundi Wu*,
	Ben Mildenhall*,
      Philipp Henzler,
      Keunhong Park,
      Ruiqi Gao,
      Daniel Watson,
      Pratul P. Srinivasan,
      Dor Verbin,
	<strong>Jonathan T. Barron</strong>,
      Ben Poole,
      Aleksander Holynski*
      <br>
      <em>CVPR</em>, 2024
      <br>
      <a href="https://reconfusion.github.io/">project page</a>
      /
      <a href="https://arxiv.org/abs/2312.02981">arXiv</a>
      <p></p>
      <p>
      Using a multi-image diffusion model as a regularizer lets you recover high-quality radiance fields from just a handful of images.
      </p>
    </td>
  </tr>

  <tr onmouseout="shinobi_stop()" onmouseover="shinobi_start()">
    <td style="padding:16px;width:20%;vertical-align:middle">
      <div class="one">
        <div class="two" id='shinobi_image'><video  width=100% muted autoplay loop>
        <source src="images/shinobi.mp4" type="video/mp4">
        Your browser does not support the video tag.
        </video></div>
        <img src='images/shinobi.jpg' width=100%>
      </div>
      <script type="text/javascript">
        function shinobi_start() {
          document.getElementById('shinobi_image').style.opacity = "1";
        }

        function shinobi_stop() {
          document.getElementById('shinobi_image').style.opacity = "0";
        }
        shinobi_stop()
      </script>
    </td>
    <td style="padding:8px;width:80%;vertical-align:middle">
      <a href="https://shinobi.aengelhardt.com/">
        <span class="papertitle">SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-Wild</span>
      </a>
      <br>
			
			Andrea[...]
			Amit Raj, 
			Mark Boss, 
			Yunzhi Zhang, 
			Abhishek Kar, 
			Yuanzhen Li, 
			Deqing Sun, 
			Ricardo Martin Brualla, 
      <strong>Jonathan T. Barron</strong>,
			[...]
			Varun Jampani
      <br>
      <em>CVPR</em>, 2024
      <br>
      <a href="https://shinobi.aengelhardt.com/">project page</a>
      /
      <a href="https://www.youtube.com/watch?v=m_5kvtlDnl4">video</a>
      /
      <a href="https://arxiv.org/abs/2401.10171">arXiv</a>
      <p></p>
      <p>
      A class-agnostic inverse rendering solution for turning in-the-wild images of an object into a relightable 3D asset.
      </p>
    </td>
  </tr>
	
    <tr onmouseout="internerf_stop()" onmouseover="internerf_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
 					  <img src='images/internerf_after.jpg' width=100%>
 					</div>
          <img src='images/internerf_before.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function internerf_start() {
            document.getElementById('internerf_image').style.opacity = "1";
          }

          function internerf_stop() {
            document.getElementById('internerf_image').style.opacity = "0";
          }
          internerf_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2406.11737">
          <span class="papertitle">InterNeRF: Scaling Radiance Fields via Parameter Interpolation</span>
        </a>
        <br>
		Clinton Wang,
		Peter Hedman,
		Polina Golland,
		<strong>Jonathan T. Barron</strong>,
		Daniel Duckworth
        <br>
        <em>CVPR Neural Rendering Intelligence</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2406.11737">arXiv</a>
        <p></p>
        <p>
        Parameter interpolation enables high-quality large-scale scene reconstruction and out-of-core training and rendering.
        </p>
      </td>
    </tr>

<tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:16px;width:20%;vertical-align:middle">
    <div class="one">
    <div class="two" id='difsurvey_image'><video  width=100% height=100% muted autoplay loop>
    <source src="images/difsurvey_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video></div>
      <img src='images/difsurvey_image.jpg' width="160">
    </div>
    <script type="text/javascript">
      function difsurvey_start() {
        document.getElementById('difsurvey_image').style.opacity = "1";
      }

      function difsurvey_stop() {
        document.getElementById('difsurvey_image').style.opacity = "0";
      }
      difsurvey_stop()
    </script>
  </td>
  <td style="padding:8px;width:80%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2310.07204">
      <span class="papertitle">State of the Art on Diffusion Models for Visual Computing
</span>
    </a>
    <br>
		Ryan Po,
		Wang Yifan,
		Vladislav Golyanik,
		Kfir Aberman,
		<strong>Jonathan T. Barron</strong>,
		Amit H. Bermano,
		Eric Ryan Chan,
		Tali Dekel,
		Aleksander Holynski,
		Angjoo Kanazawa,
		C. Karen Liu,
		Lingjie Liu,
		Ben Mildenhall,
        Matthias Nie√üner,
		Bj√∂rn Ommer,
		Christian Theobalt,
		Peter Wonka,
        Gordon Wetzstein
    <br>
	<em>Eurographics State-of-the-Art Report<em>, 2024
    <br>
    <p></p>
    <p>
    A survey of recent progress in diffusion models for images, videos, and 3D.
    </p>
  </td>
</tr>          

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='camp_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/camp.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/camp.png' width="160">
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://camp-nerf.github.io/">
          <span class="papertitle">CamP: Camera Preconditioning for Neural Radiance Fields</span>
        </a>
        <br>
        Keunhong Park,
        Philipp Henzler,
        Ben Mildenhall,
        <strong>Jonathan T. Barron</strong>,
        Ricardo Martin-Brualla
        <br>
        <em>SIGGRAPH Asia</em>, 2023
        <br>
        <a href="https://camp-nerf.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2308.10902">arXiv</a>
        <p></p>
        <p>
        Preconditioning based on camera parameterization helps NeRF and camera extrinsics/intrinsics optimize better together.
        </p>
      </td>
    </tr>
<!-- remainder of index content unchanged -->
...
</tbody></table>
</td></tr></tbody></table>
  </body>
</html>
